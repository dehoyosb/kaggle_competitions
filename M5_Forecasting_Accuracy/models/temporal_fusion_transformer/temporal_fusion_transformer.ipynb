{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T15:18:08.065928Z",
     "start_time": "2020-04-25T15:18:06.807267Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyunpack\n",
    "import math\n",
    "import json\n",
    "\n",
    "from data_formatters.m5 import M5Formatter\n",
    "from data_formatters.base import DataTypes, InputTypes\n",
    "from datasets.M5dataset import TFTDataset\n",
    "\n",
    "from models import GatedLinearUnit\n",
    "from models import GateAddNormNetwork\n",
    "from models import GatedResidualNetwork \n",
    "from models import ScaledDotProductAttention\n",
    "from models import InterpretableMultiHeadAttention\n",
    "from models import VariableSelectionNetwork\n",
    "\n",
    "from quantile_loss import QuantileLossCalculator\n",
    "from quantile_loss import NormalizedQuantileLossCalculator\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T15:18:14.706451Z",
     "start_time": "2020-04-25T15:18:08.067337Z"
    }
   },
   "outputs": [],
   "source": [
    "#m5_data = pd.read_pickle('/home/daniel/github/kaggle_competitions/M5_Forecasting_Accuracy/data/full_data.pkl')\n",
    "m5_data = pd.read_pickle('/container/home/millenium/Storage/Daniel/github/kaggle_competitions/M5_Forecasting_Accuracy/data/full_data.pkl')\n",
    "m5_data = m5_data[m5_data.d <= 1913]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T15:18:14.713260Z",
     "start_time": "2020-04-25T15:18:14.708494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46027957, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m5_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T15:18:15.070117Z",
     "start_time": "2020-04-25T15:18:14.714813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30490,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m5_data.id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-25T15:18:10.624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train-valid-test splits.\n",
      "Creating the Scalers\n",
      "Setting scalers with training data...\n"
     ]
    }
   ],
   "source": [
    "data_formatter = M5Formatter()\n",
    "train, valid, test = data_formatter.split_data(m5_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T01:08:37.609264Z",
     "start_time": "2020-04-25T00:40:54.163Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TFTDataset(train)\n",
    "valid_dataset = TFTDataset(valid)\n",
    "test_dataset = TFTDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "        # Data parameters\n",
    "        self.time_steps = int(hparams.total_time_steps)#int(params['total_time_steps'])\n",
    "        self.input_size = int(hparams.input_size)#int(params['input_size'])\n",
    "        self.output_size = int(hparams.output_size)#int(params['output_size'])\n",
    "        self.category_counts = json.loads(str(hparams.category_counts))#json.loads(str(params['category_counts']))\n",
    "        self.num_categorical_variables = len(self.category_counts)\n",
    "        self.num_regular_variables = self.input_size - self.num_categorical_variables\n",
    "        self.n_multiprocessing_workers = int(hparams.multiprocessing_workers) #int(params['multiprocessing_workers'])\n",
    "\n",
    "        # Relevant indices for TFT\n",
    "        self._input_obs_loc = json.loads(str(hparams.input_obs_loc))#json.loads(str(params['input_obs_loc']))\n",
    "        self._static_input_loc = json.loads(str(hparams.static_input_loc))#json.loads(str(params['static_input_loc']))\n",
    "        self._known_regular_input_idx = json.loads(str(hparams.known_regular_inputs))#json.loads(str(params['known_regular_inputs']))\n",
    "        self._known_categorical_input_idx = json.loads(str(hparams.known_categorical_inputs))#json.loads(str(params['known_categorical_inputs']))\n",
    "        \n",
    "        self.num_non_static_historical_inputs = self.get_historical_num_inputs()\n",
    "        self.num_non_static_future_inputs = self.get_future_num_inputs()\n",
    "        \n",
    "        self.column_definition = [\n",
    "                                  ('id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "                                  ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.TIME),\n",
    "                                  ('power_usage', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "                                  ('hour', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "                                  ('day_of_week', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "                                  ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "                                  ('categorical_id', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "                                ]\n",
    "\n",
    "        # Network params\n",
    "        self.quantiles = [0.1, 0.5, 0.9]\n",
    "#         self.use_cudnn = use_cudnn  # Whether to use GPU optimised LSTM\n",
    "        self.hidden_layer_size = int(hparams.hidden_layer_size)#int(params['hidden_layer_size'])\n",
    "        self.dropout_rate = float(hparams.dropout_rate)#float(params['dropout_rate'])\n",
    "        self.max_gradient_norm = float(hparams.max_gradient_norm)#float(params['max_gradient_norm'])\n",
    "        self.learning_rate = float(hparams.learning_rate)#float(params['learning_rate'])\n",
    "        self.minibatch_size = int(hparams.minibatch_size)#int(params['minibatch_size'])\n",
    "        self.num_epochs = int(hparams.num_epochs)#int(params['num_epochs'])\n",
    "        self.early_stopping_patience = int(hparams.early_stopping_patience)#int(params['early_stopping_patience'])\n",
    "\n",
    "        self.num_encoder_steps = int(hparams.num_encoder_steps)#int(params['num_encoder_steps'])\n",
    "        self.num_stacks = int(hparams.stack_size)#int(params['stack_size'])\n",
    "        self.num_heads = int(hparams.num_heads)#int(params['num_heads'])\n",
    "\n",
    "        # Serialisation options\n",
    "#         self._temp_folder = os.path.join(params['model_folder'], 'tmp')\n",
    "#         self.reset_temp_folder()\n",
    "\n",
    "        # Extra components to store Tensorflow nodes for attention computations\n",
    "        self._input_placeholder = None\n",
    "        self._attention_components = None\n",
    "        self._prediction_parts = None\n",
    "\n",
    "        print('*** {} params ***'.format(self.name))\n",
    "        for k in vars(hparams):\n",
    "            print('# {} = {}'.format(k, vars(hparams)[k]))\n",
    "            \n",
    "        self.train_criterion = QuantileLossCalculator(self.quantiles, self.output_size)\n",
    "        self.test_criterion = NormalizedQuantileLossCalculator(self.quantiles, self.output_size)\n",
    "\n",
    "        # Build model\n",
    "        ## Build embeddings\n",
    "        self.build_embeddings()\n",
    "        \n",
    "        ## Build Static Contex Networks\n",
    "        self.build_static_context_networks()\n",
    "        \n",
    "        ## Building Variable Selection Networks\n",
    "        self.build_variable_selection_networks()\n",
    "        \n",
    "        ## Build Lstm\n",
    "        self.build_lstm()\n",
    "        \n",
    "        ## Build GLU for after lstm encoder decoder and layernorm\n",
    "        self.build_post_lstm_gate_add_norm()\n",
    "        \n",
    "        ## Build Static Enrichment Layer\n",
    "        self.build_static_enrichment()\n",
    "        \n",
    "        ## Building decoder multihead attention\n",
    "        self.build_temporal_self_attention()\n",
    "        \n",
    "        ## Building positionwise decoder\n",
    "        self.build_position_wise_feed_forward()\n",
    "        \n",
    "        ## Build output feed forward\n",
    "        self.build_output_feed_forward()\n",
    "        \n",
    "        ## Initializing remaining weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if ('lstm' in name and 'ih' in name) and 'bias' not in name:\n",
    "                #print(name)\n",
    "                #print(p.shape)\n",
    "                torch.nn.init.xavier_uniform_(p)\n",
    "#                 torch.nn.init.kaiming_normal_(p, a=0, mode='fan_in', nonlinearity='sigmoid')\n",
    "            elif ('lstm' in name and 'hh' in name) and 'bias' not in name:\n",
    "        \n",
    "                 torch.nn.init.orthogonal_(p)\n",
    "            \n",
    "            elif 'lstm' in name and 'bias' in name:\n",
    "                #print(name)\n",
    "                #print(p.shape)\n",
    "                torch.nn.init.zeros_(p)\n",
    "        \n",
    "    def get_historical_num_inputs(self):\n",
    "        \n",
    "        obs_inputs = [i for i in self._input_obs_loc]\n",
    "        \n",
    "        known_regular_inputs = [i for i in self._known_regular_input_idx\n",
    "                                if i not in self._static_input_loc]\n",
    "            \n",
    "        known_categorical_inputs = [i for i in self._known_categorical_input_idx\n",
    "                                    if i + self.num_regular_variables not in self._static_input_loc]\n",
    "        \n",
    "        wired_embeddings = [i for i in range(self.num_categorical_variables)\n",
    "                            if i not in self._known_categorical_input_idx \n",
    "                            and i not in self._input_obs_loc]\n",
    "\n",
    "        unknown_inputs = [i for i in range(self.num_regular_variables)\n",
    "                          if i not in self._known_regular_input_idx\n",
    "                          and i not in self._input_obs_loc]\n",
    "\n",
    "        return len(obs_inputs+known_regular_inputs+known_categorical_inputs+wired_embeddings+unknown_inputs)\n",
    "    \n",
    "    def get_future_num_inputs(self):\n",
    "            \n",
    "        known_regular_inputs = [i for i in self._known_regular_input_idx\n",
    "                                if i not in self._static_input_loc]\n",
    "            \n",
    "        known_categorical_inputs = [i for i in self._known_categorical_input_idx\n",
    "                                    if i + self.num_regular_variables not in self._static_input_loc]\n",
    "\n",
    "        return len(known_regular_inputs + known_categorical_inputs)\n",
    "    \n",
    "    def build_embeddings(self):\n",
    "        self.categorical_var_embeddings = nn.ModuleList([nn.Embedding(self.category_counts[i], \n",
    "                                                                      self.hidden_layer_size) \n",
    "                                                     for i in range(self.num_categorical_variables)])\n",
    "\n",
    "        self.regular_var_embeddings = nn.ModuleList([nn.Linear(1, \n",
    "                                                              self.hidden_layer_size) \n",
    "                                                  for i in range(self.num_regular_variables)])\n",
    "\n",
    "    def build_variable_selection_networks(self):\n",
    "        \n",
    "        self.static_vsn = VariableSelectionNetwork(hidden_layer_size = self.hidden_layer_size,\n",
    "                                                   input_size = self.hidden_layer_size * len(self._static_input_loc),\n",
    "                                                   output_size = len(self._static_input_loc),\n",
    "                                                   dropout_rate = self.dropout_rate)\n",
    "        \n",
    "        self.temporal_historical_vsn = VariableSelectionNetwork(hidden_layer_size = self.hidden_layer_size,\n",
    "                                                                input_size = self.hidden_layer_size *\n",
    "                                                                        self.num_non_static_historical_inputs,\n",
    "                                                                output_size = self.num_non_static_historical_inputs,\n",
    "                                                                dropout_rate = self.dropout_rate,\n",
    "                                                                additional_context=self.hidden_layer_size)\n",
    "        \n",
    "        self.temporal_future_vsn = VariableSelectionNetwork(hidden_layer_size = self.hidden_layer_size,\n",
    "                                                            input_size = self.hidden_layer_size *\n",
    "                                                                        self.num_non_static_future_inputs,\n",
    "                                                            output_size = self.num_non_static_future_inputs,\n",
    "                                                            dropout_rate = self.dropout_rate,\n",
    "                                                            additional_context=self.hidden_layer_size)\n",
    "        \n",
    "    def build_static_context_networks(self):\n",
    "        \n",
    "        self.static_context_variable_selection_grn = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                                          dropout_rate=self.dropout_rate)\n",
    "        \n",
    "        self.static_context_enrichment_grn = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                              dropout_rate=self.dropout_rate)\n",
    "\n",
    "        self.static_context_state_h_grn = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                           dropout_rate=self.dropout_rate)\n",
    "        \n",
    "        self.static_context_state_c_grn = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                           dropout_rate=self.dropout_rate)\n",
    "        \n",
    "    def build_lstm(self):\n",
    "        self.historical_lstm = nn.LSTM(input_size = self.hidden_layer_size,\n",
    "                                       hidden_size = self.hidden_layer_size,\n",
    "                                       batch_first = True)\n",
    "        self.future_lstm = nn.LSTM(input_size = self.hidden_layer_size,\n",
    "                                   hidden_size = self.hidden_layer_size,\n",
    "                                   batch_first = True)\n",
    "        \n",
    "    def build_post_lstm_gate_add_norm(self):\n",
    "        self.post_seq_encoder_gate_add_norm = GateAddNormNetwork(self.hidden_layer_size,\n",
    "                                                                 self.hidden_layer_size,\n",
    "                                                                 self.dropout_rate,\n",
    "                                                                 activation = None)\n",
    "        \n",
    "    def build_static_enrichment(self):\n",
    "        self.static_enrichment = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                      dropout_rate = self.dropout_rate,\n",
    "                                                      additional_context=self.hidden_layer_size)\n",
    "        \n",
    "    def build_temporal_self_attention(self):\n",
    "        self.self_attn_layer = InterpretableMultiHeadAttention(n_head = self.num_heads, \n",
    "                                                               d_model = self.hidden_layer_size,\n",
    "                                                               dropout = self.dropout_rate)\n",
    "        \n",
    "        self.post_attn_gate_add_norm = GateAddNormNetwork(self.hidden_layer_size,\n",
    "                                                           self.hidden_layer_size,\n",
    "                                                           self.dropout_rate,\n",
    "                                                           activation = None)\n",
    "        \n",
    "    def build_position_wise_feed_forward(self):\n",
    "        self.GRN_positionwise = GatedResidualNetwork(self.hidden_layer_size,\n",
    "                                                     dropout_rate = self.dropout_rate)\n",
    "        \n",
    "        self.post_tfd_gate_add_norm = GateAddNormNetwork(self.hidden_layer_size,\n",
    "                                                         self.hidden_layer_size,\n",
    "                                                         self.dropout_rate,\n",
    "                                                         activation = None)\n",
    "        \n",
    "    def build_output_feed_forward(self):\n",
    "        self.output_feed_forward = torch.nn.Linear(self.hidden_layer_size, \n",
    "                                                   self.output_size * len(self.quantiles))\n",
    "         \n",
    "    def get_decoder_mask(self, self_attn_inputs):\n",
    "        \"\"\"Returns causal mask to apply for self-attention layer.\n",
    "        Args:\n",
    "        self_attn_inputs: Inputs to self attention layer to determine mask shape\n",
    "        \"\"\"\n",
    "        len_s = self_attn_inputs.shape[1]\n",
    "        bs = self_attn_inputs.shape[0]\n",
    "        mask = torch.cumsum(torch.eye(len_s), 0)\n",
    "        mask = mask.repeat(bs,1,1).to(torch.float32)\n",
    "\n",
    "        return mask.to(DEVICE)\n",
    "    \n",
    "    def get_tft_embeddings(self, regular_inputs, categorical_inputs):\n",
    "        # Static input\n",
    "        if self._static_input_loc:\n",
    "            static_regular_inputs = [self.regular_var_embeddings[i](regular_inputs[:, 0, i:i + 1]) \n",
    "                                    for i in range(self.num_regular_variables)\n",
    "                                    if i in self._static_input_loc]\n",
    "            #print('static_regular_inputs')\n",
    "            #print([print(emb.shape) for emb in static_regular_inputs])\n",
    "            \n",
    "            static_categorical_inputs = [self.categorical_var_embeddings[i](categorical_inputs[Ellipsis, i])[:,0,:] \n",
    "                                         for i in range(self.num_categorical_variables)\n",
    "                                         if i + self.num_regular_variables in self._static_input_loc]\n",
    "            #print('static_categorical_inputs')\n",
    "            #print([print(emb.shape) for emb in static_categorical_inputs])\n",
    "            static_inputs = torch.stack(static_regular_inputs + static_categorical_inputs, axis = 1)\n",
    "        else:\n",
    "            static_inputs = None\n",
    "            \n",
    "        # Target input\n",
    "        obs_inputs = torch.stack([self.regular_var_embeddings[i](regular_inputs[Ellipsis, i:i + 1])\n",
    "                                     for i in self._input_obs_loc], axis=-1)\n",
    "        \n",
    "        # Observed (a prioir unknown) inputs\n",
    "        wired_embeddings = []\n",
    "        for i in range(self.num_categorical_variables):\n",
    "            if i not in self._known_categorical_input_idx \\\n",
    "            and i not in self._input_obs_loc:\n",
    "                e = self.categorical_var_embeddings[i](categorical_inputs[:, :, i])\n",
    "                wired_embeddings.append(e)\n",
    "\n",
    "        unknown_inputs = []\n",
    "        for i in range(self.num_regular_variables):\n",
    "            if i not in self._known_regular_input_idx \\\n",
    "            and i not in self._input_obs_loc:\n",
    "                e = self.regular_var_embeddings[i](regular_inputs[Ellipsis, i:i + 1])\n",
    "                unknown_inputs.append(e)\n",
    "                \n",
    "        if unknown_inputs + wired_embeddings:\n",
    "            unknown_inputs = torch.stack(unknown_inputs + wired_embeddings, axis=-1)\n",
    "        else:\n",
    "            unknown_inputs = None\n",
    "            \n",
    "        # A priori known inputs\n",
    "        known_regular_inputs = [self.regular_var_embeddings[i](regular_inputs[Ellipsis, i:i + 1])\n",
    "                                for i in self._known_regular_input_idx\n",
    "                                if i not in self._static_input_loc]\n",
    "        #print('known_regular_inputs')\n",
    "        #print([print(emb.shape) for emb in known_regular_inputs])\n",
    "        \n",
    "        known_categorical_inputs = [self.categorical_var_embeddings[i](categorical_inputs[Ellipsis, i])\n",
    "                                    for i in self._known_categorical_input_idx\n",
    "                                    if i + self.num_regular_variables not in self._static_input_loc]\n",
    "       #print('known_categorical_inputs')\n",
    "       #print([print(emb.shape) for emb in known_categorical_inputs])\n",
    "\n",
    "        known_combined_layer = torch.stack(known_regular_inputs + known_categorical_inputs, axis=-1)\n",
    "        \n",
    "        return unknown_inputs, known_combined_layer, obs_inputs, static_inputs\n",
    "        \n",
    "    def forward(self, all_inputs):\n",
    "\n",
    "        regular_inputs = all_inputs[:, :, :self.num_regular_variables].to(torch.float)\n",
    "        #print('regular_inputs')\n",
    "        #print(regular_inputs.shape)\n",
    "        categorical_inputs = all_inputs[:, :, self.num_regular_variables:].to(torch.long)\n",
    "        #print('categorical_inputs')\n",
    "        #print(categorical_inputs.shape)\n",
    "        \n",
    "        unknown_inputs, known_combined_layer, obs_inputs, static_inputs \\\n",
    "            = self.get_tft_embeddings(regular_inputs, categorical_inputs)\n",
    "        \n",
    "        # Isolate known and observed historical inputs.\n",
    "        if unknown_inputs is not None:\n",
    "              historical_inputs = torch.cat([\n",
    "                  unknown_inputs[:, :self.num_encoder_steps, :],\n",
    "                  known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                  obs_inputs[:, :self.num_encoder_steps, :]\n",
    "              ], axis=-1)\n",
    "        else:\n",
    "              historical_inputs = torch.cat([\n",
    "                  known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                  obs_inputs[:, :self.num_encoder_steps, :]\n",
    "              ], axis=-1)\n",
    "                \n",
    "        #print('historical_inputs')\n",
    "        #print(historical_inputs.shape)\n",
    "        \n",
    "        # Isolate only known future inputs.\n",
    "        future_inputs = known_combined_layer[:, self.num_encoder_steps:, :]\n",
    "        #print('future_inputs')\n",
    "        #print(future_inputs.shape)\n",
    "              \n",
    "        #print('static_inputs')\n",
    "        #print(static_inputs.shape)\n",
    "        \n",
    "        static_encoder, sparse_weights = self.static_vsn(static_inputs)\n",
    "        \n",
    "        #print('static_encoder')\n",
    "        #print(static_encoder.shape)\n",
    "        \n",
    "        #print('sparse_weights')\n",
    "        #print(sparse_weights.shape)\n",
    "        \n",
    "        static_context_variable_selection = self.static_context_variable_selection_grn(static_encoder)\n",
    "        #print('static_context_variable_selection')\n",
    "        #print(static_context_variable_selection.shape)\n",
    "        static_context_enrichment = self.static_context_enrichment_grn(static_encoder)\n",
    "        #print('static_context_enrichment')\n",
    "        #print(static_context_enrichment.shape)\n",
    "        static_context_state_h = self.static_context_state_h_grn(static_encoder)\n",
    "        #print('static_context_state_h')\n",
    "        #print(static_context_state_h.shape)\n",
    "        static_context_state_c = self.static_context_state_c_grn(static_encoder)\n",
    "        #print('static_context_state_c')\n",
    "        #print(static_context_state_c.shape)\n",
    "        \n",
    "        historical_features, historical_flags \\\n",
    "        = self.temporal_historical_vsn((historical_inputs,\n",
    "                                        static_context_variable_selection))\n",
    "        #print('historical_features')\n",
    "        #print(historical_features.shape)\n",
    "        #print('historical_flags')\n",
    "        #print(historical_flags.shape)\n",
    "        \n",
    "        future_features, future_flags \\\n",
    "        = self.temporal_future_vsn((future_inputs,\n",
    "                                    static_context_variable_selection))\n",
    "        #print('future_features')\n",
    "        #print(future_features.shape)\n",
    "        #print('future_flags')\n",
    "        #print(future_flags.shape)\n",
    "        \n",
    "        history_lstm, (state_h, state_c) \\\n",
    "        = self.historical_lstm(historical_features,\n",
    "                               (static_context_state_h.unsqueeze(0),\n",
    "                                static_context_state_c.unsqueeze(0)))\n",
    "        #print('history_lstm')\n",
    "        #print(history_lstm.shape)\n",
    "        #print('state_h')\n",
    "        #print(state_h.shape)\n",
    "        #print('state_c')\n",
    "        #print(state_c.shape)\n",
    "        \n",
    "        future_lstm, _ = self.future_lstm(future_features,\n",
    "                                          (state_h,\n",
    "                                           state_c))\n",
    "        #print('future_lstm')\n",
    "        #print(future_lstm.shape)\n",
    "        \n",
    "        # Apply gated skip connection\n",
    "        input_embeddings = torch.cat((historical_features, future_features), axis=1)\n",
    "        #print('input_embeddings')\n",
    "        #print(input_embeddings.shape) \n",
    "        \n",
    "        lstm_layer = torch.cat((history_lstm, future_lstm), axis=1)\n",
    "        #print('lstm_layer')\n",
    "        #print(lstm_layer.shape) \n",
    "        \n",
    "        temporal_feature_layer = self.post_seq_encoder_gate_add_norm(lstm_layer, input_embeddings)\n",
    "        #print('temporal_feature_layer')\n",
    "        #print(temporal_feature_layer.shape)  \n",
    "        \n",
    "        # Static enrichment layers\n",
    "        expanded_static_context = static_context_enrichment.unsqueeze(1)\n",
    "        \n",
    "        enriched = self.static_enrichment((temporal_feature_layer, expanded_static_context))\n",
    "        #print('enriched')\n",
    "        #print(enriched.shape)    \n",
    "        \n",
    "        # Decoder self attention\n",
    "        #self.mask = self.get_decoder_mask(enriched)\n",
    "        #print('enriched')\n",
    "        #print(enriched.shape)\n",
    "        x, self_att = self.self_attn_layer(enriched, \n",
    "                                           enriched, \n",
    "                                           enriched,\n",
    "                                           mask = self.get_decoder_mask(enriched))\n",
    "        #print('x')\n",
    "        #print(x.shape)\n",
    "        #print('self_att')\n",
    "        #print(self_att.shape)\n",
    "        \n",
    "        x = self.post_attn_gate_add_norm(x, enriched)\n",
    "        #print('x')\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Nonlinear processing on outputs\n",
    "        decoder = self.GRN_positionwise(x)\n",
    "        #print('decoder')\n",
    "        #print(decoder.shape)\n",
    "        \n",
    "        # Final skip connection\n",
    "        transformer_layer = self.post_tfd_gate_add_norm(decoder, temporal_feature_layer)\n",
    "        #print('transformer_layer')\n",
    "        #print(transformer_layer.shape)\n",
    "        \n",
    "        outputs = self.output_feed_forward(transformer_layer[Ellipsis, self.num_encoder_steps:, :])\n",
    "        #print('outputs')\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        return self.train_criterion.apply(y_hat, y)\n",
    "    \n",
    "    def test_loss(self, y_hat, y):\n",
    "        return self.test_criterion.apply(y_hat, y, self.quantiles[1])\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y, _ = batch\n",
    "        \n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "#         print('y')\n",
    "#         print(y.shape)\n",
    "        y_hat = self.forward(x)\n",
    "#         print('y_hat')\n",
    "#         print(y_hat.shape)\n",
    "        loss = self.loss(y_hat, torch.cat([y, y, y], dim = -1))\n",
    "        #print(loss.shape)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y, _ = batch\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "        y_hat = self.forward(x)\n",
    "        #print(y_hat.shape)\n",
    "        #print(torch.cat([y, y, y], dim = -1).shape)\n",
    "        loss = self.loss(y_hat, torch.cat([y, y, y], dim = -1))\n",
    "        #print(loss)\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y, _ = batch\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "        y_hat = self.forward(x)\n",
    "        return {'test_loss': self.test_loss(y_hat[Ellipsis, 1], y[Ellipsis, 0])}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        return [torch.optim.Adam(self.parameters(), lr=self.learning_rate)]\n",
    "    \n",
    "    def plot_grad_flow(self, named_parameters):\n",
    "        ave_grads = []\n",
    "        layers = []\n",
    "        for name, p in named_parameters:\n",
    "            if p.grad is not None:\n",
    "                if (p.requires_grad) and (\"bias\" not in name):\n",
    "                    layers.append(name)\n",
    "                    ave_grads.append(p.grad.abs().mean())\n",
    "                    self.logger.experiment.add_histogram(tag=name, values=p.grad,\n",
    "                                                         global_step=self.trainer.global_step)\n",
    "            else:\n",
    "                 print('{} - {}'.format(name, p.requires_grad))\n",
    "            \n",
    "        plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "        plt.hlines(0, 0, len(ave_grads), linewidth=1, color=\"k\" )\n",
    "        plt.xticks(list(range(0,len(ave_grads), 1)), layers, rotation='vertical')\n",
    "        plt.xlim(left=0, right=len(ave_grads))\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"average gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        # example to inspect gradient information in tensorboard\n",
    "        if self.trainer.global_step % 25 == 0:  \n",
    "            self.plot_grad_flow(self.named_parameters())\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(train_dataset, batch_size = self.minibatch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(valid_dataset, batch_size = self.minibatch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(test_dataset, batch_size = self.minibatch_size, shuffle=True, drop_last=True, num_workers=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
